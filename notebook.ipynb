{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cc63bd94-55db-4e28-8625-04895bf11ff0",
    "_uuid": "76761582c07062c8c5d6a5141dc8f1f3100719f3"
   },
   "source": [
    "#  **Flights_Analysis_in_2015** \n",
    "Mengran Tang (December 2017)\n",
    "___\n",
    "\n",
    "This notebook analyze the **flights distribution** across US in 2015 for different major airlines. Then it develop a **lightGBM regression model** to predict flights delays after analyzing them in order to give advice on reducing delays. Finally, it solve a **natural language processing problem** using recurrent neural network for airlines sentiment analysis accoring to the Twitter comments.\n",
    "\n",
    "____\n",
    "From a **_technical point of view_**, the main aspects of python covered throughout the notebook are:\n",
    "- **visualization**: matplolib, seaborn, plotly\n",
    "- **data manipulation**: pandas, numpy\n",
    "- **modeling**: keras, LightGBM\n",
    "- **problem**: regression, natural language processing\n",
    "___\n",
    "Two parts of data are involved in this notebook. First, the flight delay data was collected and published by the DOT's Bureau of Transportation Statistics, which tracks the on-time performance of domestic flights operated by large air carriers. Second, the Twitter sentiment data came from Crowdflower's Data for Everyone library, which is scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\")\n",
    "\n",
    "Plotly is the main tool for data visualization in order to get beautiful pictures as well as interactivity. Clicking and draging on the graph can provide detail information on plotly graph. LightGBM has been proven that it is not just quick but also accurate compare to other methods such as XGBoost, so I think it is good for this regression problem. As for dealing with the Twitter, Natural language processing is needed. Because it is well known that RNN is widely used in NLP such as translate, using keras to implement a RNN model to solve this sentiment analysis problem should be fine. <br>\n",
    "___\n",
    "\n",
    "# Table of Contents\n",
    "This notebook is composed of three parts: Flights and Airlines data visualization (section 1), Delay Analysis and Predicting (section 2) and Twitter Sentiment Analysis (section 3).\n",
    "\n",
    "* [** _Preamble_:** _overview of the datasets_](#0) <br>\n",
    "\n",
    "\n",
    "* [**1. Flights and Airlines Data Visualization**](#1) <br>\n",
    "   * [1.1 Flights Overall Situation in US](#11) <br>\n",
    "   * [1.2 Flights Distribution in Airlines and Cities](#12) <br>\n",
    "   * [1.3 Flights Among States](#13) <br>\n",
    "   * [1.4 Monthly Changes of Flights](#14) <br>\n",
    "   * [1.5 Flights Distance Over City and Airline](#15) <br>\n",
    "   * [1.6 Canceled Flights Situation](#16) <br>\n",
    "<br>\n",
    "* [**2. Delay Analysis and Predicting**](#2) <br>\n",
    "   * [2.1 Different Delays Correlation](#21) <br>\n",
    "   * [2.2 Basic Statistical Description of Delays](#22) <br>\n",
    "   * [2.3 Delays Heatmap](#23) <br>\n",
    "   * [2.4 Delays Distribution](#24) <br>\n",
    "   * [2.5 Delays Type](#25) <br>\n",
    "   * [2.6 Speed Analysis](#26) <br>\n",
    "   * [2.7 Build and Train Model](#27) <br>\n",
    "   * [2.8 Model Evaluation](#28) <br>\n",
    "   * [2.9 Factor Analysis and Suggestion](#29) <br>\n",
    "<br>   \n",
    "* [**3. Twitter Sentiment Analysis**](#3) <br>\n",
    "   * [3.1 Sentiment Distribution](#31) <br>\n",
    "   * [3.2 Word Cloud for Positive Sentiment](#32) <br>\n",
    "   * [3.3 Word Cloud for Negative Sentiment](#33) <br>\n",
    "   * [3.4 Word Cloud for Neutral Sentiment](#34) <br>\n",
    "   * [3.5 Build and Train RNN Model](#35) <br>\n",
    "<br>\n",
    "* [**Conclusion**](#4) <br>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d22ca32-425e-4ccc-9b39-3dbdf1054bf5",
    "_uuid": "125a98ba476b33f84a1e31a98a3dd5201d06ee75"
   },
   "source": [
    "![](http://www.slate.com/content/dam/slate/articles/technology/future_tense/2016/05/160503_FT_cybersecurity-airplanes.jpg.CROP.promo-xlarge2.jpg)\n",
    "<a id=\"0\"></a>\n",
    "___\n",
    "## _Preamble_: overview of the dataset\n",
    "\n",
    "First, load all the packages that will be needed during this project, and then, read all files that contains the details of all the flights that occured in 2015. Now, I output some informations about the flights concerning the types of the variables in the dataframe and the quantity of null values for each variable.\n",
    "\n",
    "PS: Click the bottom below to show the raw code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "9fa241b0-b5e7-4d3b-9edc-7fb2369c0e2c",
    "_uuid": "702f6a89b84f74367c5c864045fb6afe36ce17ec"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2c20714deea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msquarify\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m from .callback import (early_stopping, print_evaluation, record_evaluation,\n\u001b[0;32m     10\u001b[0m                        reset_parameter)\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m from .compat import (DataFrame, Series, integer_types, json,\n\u001b[0m\u001b[0;32m     17\u001b[0m                      \u001b[0mjson_default_with_numpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                      string_type)\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\lightgbm\\compat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;34m\"\"\"sklearn\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m from .validation import (as_float_array,\n\u001b[0m\u001b[0;32m     12\u001b[0m                          \u001b[0massert_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                          \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNonBLASDotWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_scipy_sparse_lsqr_backport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0misolve\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdsolve\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\linalg\\isolve\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0miterative\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mminres\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mminres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlgmres\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlgmres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsqr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsmr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsmr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\linalg\\isolve\\lgmres.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqr_insert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstsq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_system\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlinalg_version\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg_version\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdecomp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\misc.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mblas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'LinAlgError'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\lapack.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mblas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfind_best_blas_type\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfind_best_lapack_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_flapack\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_clapack\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import math\n",
    "import squarify\n",
    "import plotly.offline as py\n",
    "py.offline.init_notebook_mode()\n",
    "pd.options.display.max_columns = 50\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input,Dropout,Dense,BatchNormalization,Activation,concatenate,GRU,Embedding,Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras import backend as K\n",
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<script>\n",
    "  code_show=true; \n",
    "  function code_toggle() {\n",
    "   if (code_show){\n",
    "     $('div.input').hide();\n",
    "   } else {\n",
    "     $('div.input').show();\n",
    "   }\n",
    "   code_show = !code_show;\n",
    "  } \n",
    "  $(document).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\">\n",
    "  <input type=\"submit\" value=\"Click here to toggle on/off the raw code.\">\n",
    "</form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e86def9a-e44b-4968-b729-dbfaf214ec2b",
    "_uuid": "2c915715e785b709c9f50a00c66b7f4697bf96eb"
   },
   "outputs": [],
   "source": [
    "airlines = pd.read_csv('../input/flight-delays/airlines.csv')\n",
    "airports = pd.read_csv('../input/flight-delays/airports.csv')\n",
    "flights = pd.read_csv('../input/flight-delays/flights.csv',low_memory=False)\n",
    "tweet = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv', parse_dates=['tweet_created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b5e8d6ec-fb9c-43ff-b5a2-ca9c40323ccd",
    "_uuid": "5a8dfd872684f24581e9f3d90ecc9542a621b618"
   },
   "outputs": [],
   "source": [
    "print('Flights Data dimensions:', flights.shape)\n",
    "tab_info=pd.DataFrame(flights.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(flights.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(flights.isnull().sum()/flights.shape[0]*100)\n",
    "                         .T.rename(index={0:'null values (%)'}))\n",
    "tab_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76d889e0-0529-457b-abe2-3e18387f6632",
    "_uuid": "387f75658e7e0fa9b2c1f37d1aba96edb8632a43"
   },
   "source": [
    "Each entry of the `flights.csv` file corresponds to a flight and we see that more than 5'800'000 flights have been recorded in 2015. These flights are described according to 31 variables. I briefly recall the meaning of the variables that will be used in this notebook:\n",
    "\n",
    "- **YEAR, MONTH, DAY, DAY_OF_WEEK**: dates of the flight <br/>\n",
    "- **AIRLINE**: An identification number assigned by US DOT to identify a unique airline <br/>\n",
    "- **ORIGIN_AIRPORT** and **DESTINATION_AIRPORT**: code attributed by IATA to identify the airports <br/>\n",
    "- **SCHEDULED_DEPARTURE** and **SCHEDULED_ARRIVAL** : scheduled times of take-off and landing <br/> \n",
    "- **DEPARTURE_TIME** and **ARRIVAL_TIME**: real times at which take-off and landing took place <br/> \n",
    "- **DEPARTURE_DELAY** and **ARRIVAL_DELAY**: difference (in minutes) between planned and real times <br/> \n",
    "- **SECURITY_DELAY** and **AIRLINE_DELAY**:  delay caused by security and airline<br/> \n",
    "- **LATE_AIRCRAFT_DELAY** and **WEATHER_DELAY**:  delay caused by aircraft and weather<br/> \n",
    "- **AIR_SYSTEM_DELAY** and **DISTANCE**: delay caused by air system and distance (in miles)  <br/>\n",
    "\n",
    "Now, let's get some information about the Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "359b4a76-9ee4-486e-a4d6-49109f3509c1",
    "_uuid": "2180fa174791c48bbd0012cfbae436e4ba654099"
   },
   "outputs": [],
   "source": [
    "print('Twitter Data dimensions:', tweet.shape)\n",
    "tab_info=pd.DataFrame(tweet.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(tweet.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(tweet.isnull().sum()/tweet.shape[0]*100)\n",
    "                         .T.rename(index={0:'null values (%)'}))\n",
    "tab_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0119d5e5-8f4e-44ea-94ed-63594d810ffa",
    "_uuid": "a0b7729874d8c633a3369f61b246e74c5fdff15a"
   },
   "source": [
    "Each entry of the `tweet.csv` file corresponds to a tweet and we see that more than 14,000 tweets have been recorded in 2015. These flights are described according to 15 variables. I explain meaning of each variables which will be used in this notebook:\n",
    "\n",
    "- **airline_sentiment** and **airline_sentiment_confidence**: the customer sentiment and confidence for this airline <br/>\n",
    "- **negativereason** and **negativereason_confidence**: reason and confidence for this nagetive sentiment <br/>\n",
    "- **airline** and **retweet_count**: airline name and retweet count<br/> \n",
    "- **text** and **tweet_location**: the content and location of the tweet  <br/>\n",
    "- **user_timezone** and **tweet_created**: the timezone of the user and the time when this tweet was created<br/>\n",
    "\n",
    "The `airlines.csv` file gives us the airline abreviations and its corresponding airline name. The visualizations below use the abreviation instead of full name, since they are much shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dfb33ce3-6dad-4a2f-9e4f-974a06cc050c",
    "_uuid": "3f2e0c320ffc0da38277e55fa0969503caf01bcc"
   },
   "outputs": [],
   "source": [
    "airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c592894-4b8e-462a-8dd6-44075b2570fa",
    "_uuid": "4a501dc633fc4dca505445ab69151c8c628a8e9c"
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "___\n",
    "## 1. Flights and Airlines Data Visualization\n",
    "___\n",
    "<a id=\"11\"></a>\n",
    "### 1.1 Flights Overall Situation in US\n",
    "\n",
    "To have a global overview of the geographical data covered in this dataset, we can plot the airports location and the flight path between them on the world map. The size of markers indicate the count of flights which took off at this airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3d5c7d7e-ff8b-4d55-9496-94a4dbdaac0c",
    "_uuid": "8d570713677c51d6ffcabbdb411550f39c3e1ecf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flights = flights.merge(airports, left_on='ORIGIN_AIRPORT', right_on='IATA_CODE')\n",
    "flights = flights.rename(columns={'AIRPORT':'ORIGIN_AIRPORT_NAME','CITY':'ORIGIN_CITY','STATE':'ORIGIN_STATE',\n",
    "                                         'LATITUDE':'ORIGIN_LATITUDE','LONGITUDE':'ORIGIN_LONGITUDE'})\n",
    "flights = flights.drop(['COUNTRY','IATA_CODE'],axis=1)\n",
    "flights = flights.merge(airports, left_on='DESTINATION_AIRPORT', right_on='IATA_CODE')\n",
    "flights = flights.rename(columns={'AIRPORT':'DESTINATION_AIRPORT_NAME','CITY':'DESTINATION_CITY',\n",
    "                            'STATE':'DESTINATION_STATE','LATITUDE':'DESTINATION_LATITUDE','LONGITUDE':'DESTINATION_LONGITUDE'})\n",
    "flights = flights.drop(['COUNTRY','IATA_CODE'],axis=1)\n",
    "count = flights.ORIGIN_AIRPORT.value_counts().reset_index()\n",
    "count.columns = ['IATA_CODE','COUNT']\n",
    "airports = airports.merge(count, on='IATA_CODE')\n",
    "df_path = flights[['DESTINATION_LONGITUDE', 'DESTINATION_LATITUDE','ORIGIN_LONGITUDE','ORIGIN_LATITUDE']]\n",
    "df_path = df_path.drop_duplicates()\n",
    "df_path = df_path.reset_index()\n",
    "AIRPORT = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        lon = airports['LONGITUDE'],\n",
    "        lat = airports['LATITUDE'],\n",
    "        hoverinfo = 'text',\n",
    "        text = airports['AIRPORT'],\n",
    "        mode = 'markers',\n",
    "        marker = dict( \n",
    "            size=np.log10(airports['COUNT'])/np.log10(5)*1.2, \n",
    "            color=airports['COUNT'],\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(\n",
    "            thickness=10,\n",
    "            titleside='right',\n",
    "            outlinecolor='rgba(68,68,68,0)',\n",
    "            ticks='outside',\n",
    "            ticklen= 3,\n",
    "            ticksuffix=' flights count',\n",
    "            dtick= 30000\n",
    "             ),\n",
    "            line = dict(\n",
    "                width=3,\n",
    "                color='rgba(68, 68, 68, 0)'\n",
    "            )\n",
    "        ))]\n",
    "flight_paths = []\n",
    "for i in range(len(df_path)):\n",
    "    flight_paths.append(dict(\n",
    "            type = 'scattergeo',\n",
    "            lon = [df_path['ORIGIN_LONGITUDE'][i], df_path['DESTINATION_LONGITUDE'][i]],\n",
    "            lat = [df_path['ORIGIN_LATITUDE'][i], df_path['DESTINATION_LATITUDE'][i]],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 0.09,\n",
    "                color = 'red',\n",
    "            ),\n",
    "            opacity = 0.5,\n",
    "        ))\n",
    "layout = dict(\n",
    "        title = 'Flights Count and Distribution across US in 2015<br>(Click and drag to move and use wheel to zoom in)',\n",
    "        titlefont={\"size\": 26},\n",
    "        autosize=True,\n",
    "        showlegend = False,\n",
    "        geo = dict(\n",
    "            resolution = 50,\n",
    "            showland = True,\n",
    "            showlakes = True,\n",
    "            landcolor = 'rgb(204, 204, 204)',\n",
    "            countrycolor = 'rgb(204, 204, 204)',\n",
    "            lakecolor = 'rgb(255, 255, 255)',\n",
    "            projection = dict( type=\"equirectangular\" ),\n",
    "            coastlinewidth = 2,\n",
    "            lataxis = dict(\n",
    "                range = [ 25, 50],\n",
    "                showgrid = True,\n",
    "                tickmode = \"linear\",\n",
    "                dtick = 10\n",
    "            ),\n",
    "            lonaxis = dict(\n",
    "                range = [-125, -69],\n",
    "                showgrid = True,\n",
    "                tickmode = \"linear\",\n",
    "                dtick = 20\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "fig = dict( data=flight_paths + AIRPORT, layout=layout )\n",
    "py.iplot( fig, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dfab79a6-4c70-4bcf-aca0-d18ecadf3092",
    "_uuid": "0c065deb8b9c68c9caa546da9d111e28b56f1f9d"
   },
   "source": [
    "#### Analysis:  Big cities such as Chicago, Atlanta, San Francisco and New York have numerous flights took off in 2015 while the north-west and north part of America have least flights to take off. From general, airports located in US east coast are busier than airports in west coast.\n",
    "\n",
    "<a id=\"12\"></a>\n",
    "### 1.2 Flights Distribution in Airlines and Cities\n",
    "\n",
    "The following two pie charts give the percentage of flights for each airline and city. Here, the second pie chart only consider the top 14 cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be26c72b-eb21-4659-80f7-90c95d71f7a7",
    "_uuid": "f4731f491c6b797ff9dbaab0c2bf051ed91706d9"
   },
   "outputs": [],
   "source": [
    "def calculateTextpositions(values):\n",
    "    total = sum(values)\n",
    "    return values.apply(lambda v: 'none' if float(v)/total < 0.01 else 'auto')\n",
    "COUNT_AIRLINE = flights.AIRLINE.value_counts().reset_index()\n",
    "COUNT_AIRLINE.columns = ['AIRLINE_NAME','COUNT']\n",
    "COUNT_CITY = flights.ORIGIN_CITY.value_counts().reset_index()\n",
    "COUNT_CITY.columns = ['CITY_NAME','COUNT']\n",
    "trace1 = go.Pie(labels=list(COUNT_AIRLINE['AIRLINE_NAME']),\n",
    "            values=list(COUNT_AIRLINE['COUNT']),\n",
    "            domain = {\"x\": [0, 0.47]},\n",
    "            name = 'AIRLINE',\n",
    "            textinfo = \"label+percent\",\n",
    "            textfont = {\"size\" : 8},\n",
    "            textposition =calculateTextpositions(COUNT_AIRLINE['COUNT']),\n",
    "            legendgroup='group1',\n",
    "            hole= .4)\n",
    "trace2 = go.Pie(labels=list(COUNT_CITY.head(14)['CITY_NAME']),\n",
    "            values=list(COUNT_CITY.head(14)['COUNT']),\n",
    "            domain = {\"x\": [0.53, 1]},\n",
    "            name = 'CITY',\n",
    "            legendgroup='group2',\n",
    "            textinfo = \"label+percent\",\n",
    "            textfont = {\"size\" : 8},\n",
    "            textposition =calculateTextpositions(COUNT_CITY.head(14)['COUNT']),\n",
    "            hole= .4)\n",
    "layout = go.Layout(\n",
    "    title=\"Distributions for Different Flights\",\n",
    "    titlefont={\"size\": 26},\n",
    "    legend=dict(orientation=\"h\",x=-.3, y=1.2,font=dict(size=6)),\n",
    "    annotations = [\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 25\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"AIRLINE\",\n",
    "                \"x\": 0.155,\n",
    "                \"y\": 0.5\n",
    "            },\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 25\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"CITY\",\n",
    "                \"x\": 0.808,\n",
    "                \"y\": 0.5\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "fig = go.Figure(data=[trace1,trace2], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92ac267bbef26fbc050328fdcd1eb30e60c71bc2"
   },
   "source": [
    "#### Analysis:  From the first pie chart, we see that there is some disparity between the carriers. For exemple, Southwest Airlines accounts for  ∼∼ 20% of the flights which is similar to the number of flights chartered by the 7 tiniest airlines. Now, if we have a look at the second pie chart, we see that here the differences of flights percentage among cities are less pronounced. Chicago and Atlanta are the most popular cities in US, each of them is nearlly 3 times than Seattle.\n",
    "\n",
    "<a id=\"13\"></a>\n",
    "### 1.3 Flights Among States\n",
    "\n",
    "Now flights is a grouping to states as we can see from the latter analysis, there are a total of 51 distinct states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e519338407e3a74b3d3c4991d18e2e8a68364df"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 21))\n",
    "marrimeko=flights.ORIGIN_STATE.value_counts().to_frame()\n",
    "ax = fig.add_subplot(111, aspect=\"equal\")\n",
    "ax = squarify.plot(sizes=marrimeko['ORIGIN_STATE'].values,label=marrimeko.index,\n",
    "              color=sns.color_palette('cubehelix_r', 28), alpha=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(40,25)\n",
    "plt.title(\"Treemap of Flights Counts Across different States\", fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e0eaf2f7a58729935379c2b73d3845a1e64ff2"
   },
   "source": [
    "#### Analysis:  CA, TX and FL have the most flights took off, followed by IL, GA and NY.\n",
    "\n",
    "<a id=\"14\"></a>\n",
    "### 1.4 Monthly Changes of Flights\n",
    "\n",
    "Let's see the change of flights from January to December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8e440b970a9c327bc4c4cbd855a4354f9135710"
   },
   "outputs": [],
   "source": [
    "count_month = flights.groupby(['MONTH', 'AIRLINE']).count().reset_index()[['MONTH','AIRLINE','YEAR']]\n",
    "data = []\n",
    "for name in count_month['AIRLINE'].unique():\n",
    "    trace = go.Scatter(\n",
    "        x = count_month[count_month['AIRLINE']==name]['MONTH'],\n",
    "        y = count_month[count_month['AIRLINE']==name]['YEAR'],\n",
    "        mode = 'lines+markers',\n",
    "        name = name\n",
    "    )\n",
    "    data.append(trace)\n",
    "\n",
    "layout = dict(title = 'Monthly Changes of Flights',\n",
    "              xaxis = dict(title = 'Month'),\n",
    "              yaxis = dict(title = 'Flghts Count'),\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7def812673420f02626df8a2882d2e0fab473cc"
   },
   "source": [
    "#### Analysis:  Almost all airlines have lowest flights in February because of the weather. American Airlines Inc. has a great growth in flights counts from June to July.\n",
    "\n",
    "<a id=\"15\"></a>\n",
    "### 1.5 Flights Distance Over City and Airline\n",
    "\n",
    "Now, the next 3D plot shows the flights distance distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74c56e611b0bda69cb299c585ba1abbc975e7c17"
   },
   "outputs": [],
   "source": [
    "from plotly.graph_objs import *\n",
    "TOP_AIRLINE = np.array(flights.AIRLINE.value_counts().index[:8])\n",
    "TOP_CITY = np.array(flights.ORIGIN_CITY.value_counts().index[:8])\n",
    "flights_sample = flights[flights['AIRLINE'].isin(TOP_AIRLINE)]\n",
    "flights_sample = flights_sample[flights_sample['ORIGIN_CITY'].isin(TOP_CITY)]\n",
    "flights_sample = flights_sample[flights.CANCELLED==0]\n",
    "flights_sample = flights_sample.drop_duplicates(subset='DISTANCE')\n",
    "trace1 = Scatter3d(\n",
    "    x=flights_sample['AIRLINE'],\n",
    "    y=flights_sample['ORIGIN_CITY'],\n",
    "    z=flights_sample['DISTANCE'],\n",
    "    text=flights_sample['FLIGHT_NUMBER'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 750,\n",
    "        size= flights_sample['DISTANCE']*4,\n",
    "        color = flights_sample['DISTANCE'],\n",
    "        colorscale = 'Viridis',\n",
    "        colorbar = dict(title = 'Distance<br>Longth'),\n",
    "        line=dict(color='rgb(140, 140, 170)')\n",
    "    )\n",
    ")\n",
    "data=[trace1]\n",
    "layout=dict(height=800, width=800, title='Flights Distance Over City and Airline')\n",
    "fig=dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b493bab2-10d6-412d-a53a-ca49be6f4fa5",
    "_uuid": "49d7c8b1a1cb4df54b2b9b641285aa1ecae65f93"
   },
   "source": [
    "#### Analysis: Delta Air Lines Inc. has the longest trip flight, while United Air Lines Inc. and American Airlines Inc. have many long distance flights too. Los Angeles and San Francisco have no very long distance flights, since they are located in west coast of US.\n",
    "\n",
    "<a id=\"16\"></a>\n",
    "### 1.6 Canceled Flights Situation\n",
    "\n",
    "As for flights canceled in 2015, there are 4 reasons for cancellation. This bar chart shows the frequency of different reasons per airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dfe727f2-3a38-4e54-8ec1-815c31276343",
    "_uuid": "7a83769627ec0b1e83597dd5b69f5d0422e92923"
   },
   "outputs": [],
   "source": [
    "CANCELLED_FLIGHTS = flights[flights.CANCELLED==1]\n",
    "AC = CANCELLED_FLIGHTS.AIRLINE.value_counts().reset_index()\n",
    "AC.columns=['airline','count']\n",
    "def decide_reason(x):\n",
    "    if x == 'A':\n",
    "        return 'Airline/Carrier'\n",
    "    elif x =='B':\n",
    "        return 'Weather'\n",
    "    elif x=='C':\n",
    "        return 'National Air System'\n",
    "    else:\n",
    "        return 'Security'\n",
    "CANCELLED_FLIGHTS['Cancle Reason'] = flights['CANCELLATION_REASON'].apply(decide_reason)\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = sns.countplot(data = CANCELLED_FLIGHTS, x = 'AIRLINE', hue='Cancle Reason', order=AC.airline)\n",
    "ax.set_title('Canceled Flights Count For Different Airline',size=30)\n",
    "ax.set_ylabel('Count',size=20)\n",
    "ax.set_xlabel('Airline',size=20)\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "187246e3-d3d0-4106-a0cb-4d5668e53ab2",
    "_uuid": "0edf5e6dd5a7e677190ed667a2cd09050594eb37"
   },
   "source": [
    "###### Analysis:  We can find that American Eagle Airlines Inc. have only 5.11% flights counts from last pie chart, but it has nearlly same canceled flights as Southwest Airlines, which shows that American Eagle Airlines Inc. has the highest chance to cancel its flights. Among the cancellation reasons, weather is the most common reason except for Atlantic Southeast Airlines, Virgin America and Hawaiian Airlines Inc.\n",
    "<a id=\"2\"></a>\n",
    "___\n",
    "## 2. Delay Analysis and Predicting\n",
    "___\n",
    "<a id=\"21\"></a>\n",
    "### 2.1 Different Delays Correlation\n",
    "\n",
    "There are different kinds of delay. So, I plot a heatmap to show the pearson correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ca651a62-3411-4709-afdf-942d72cb9f79",
    "_uuid": "734264d998922be1704f8c503346905c90af5f33"
   },
   "outputs": [],
   "source": [
    "flights = flights[flights.CANCELLED==0]\n",
    "flights = flights.fillna(0)\n",
    "different_type_delay = flights[['DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY' , 'AIRLINE_DELAY', \n",
    "                               'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY']]\n",
    "colormap = plt.cm.magma\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.title('Pearson Correlation of Different Delays', y=1.05, size=30)\n",
    "sns.heatmap(different_type_delay.corr(),linewidths=0.1,vmax=1.0, square=True, \n",
    "            cmap=colormap, linecolor='white', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3cfc2b66-538c-424c-b948-33650c44ad12",
    "_uuid": "ca7990285a708c99da1caff63e65e9c5671f89be"
   },
   "source": [
    "#### Analysis:  Arrival delay has high relation with departure delay. But it has not so high dependecy with other type of delay such as security delay. Air system delay has the least relation with other delays.\n",
    "<a id=\"22\"></a>\n",
    "### 2.2 Basic Statistical Description of Delays  \n",
    "\n",
    "Here, the aim is to classify the airlines with respect to their punctuality and for that purpose. I compute a few basic statisticial parameters.\n",
    "\n",
    "This chart only considers the arrival delay, since I think it is the most important delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1da45409-8c1e-4ab7-8805-8bea1fdb5774",
    "_uuid": "ad9aa45e316df08cec51a9e10c7f81635c96b59d"
   },
   "outputs": [],
   "source": [
    "def get_stats(group):\n",
    "    return {'min': group.min(), 'max': group.max(),\n",
    "            'count': group.count(), 'mean': group.mean()}\n",
    "\n",
    "global_stats = flights['ARRIVAL_DELAY'].groupby(flights['AIRLINE']).apply(get_stats).unstack()\n",
    "global_stats = global_stats.sort_values('count',ascending=False)\n",
    "global_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "962f5056-6170-4d73-85e6-2191bfaed64f",
    "_uuid": "f285940b4a638016628384ecba50acc419b52fbc"
   },
   "source": [
    "#### Analysis:  Spirit Air Lines and Frontier Airlines Inc. have the highest mean delay,  meaning that you will lose your time if you choose them. However, note that  Alaska Airlines Inc.'s mean delay is quite low that the standard for its flights is to respect the schedule.\n",
    "<a id=\"23\"></a>\n",
    "### 2.3 Delays Heatmap \n",
    "\n",
    "Now, I consider all the flights from carriers which have high mean arrival delay. In order to facilitate the delay distribution information, I construct the heatmap between carriers and city, trying to define if there is a correlation between the delays registered and the city of origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c4c2b71e-5174-4427-9c6f-24f3fddf7b02",
    "_uuid": "600661c38933afa6ef5359945b6b575f19605a37"
   },
   "outputs": [],
   "source": [
    "airport_mean_delays = pd.DataFrame(pd.Series(flights['ORIGIN_AIRPORT'].unique()))\n",
    "airport_mean_delays.set_index(0, drop = True, inplace = True)\n",
    "identify_airport = airports.set_index('IATA_CODE')['CITY'].to_dict()\n",
    "abbr_companies = airlines.set_index('IATA_CODE')['AIRLINE'].to_dict()\n",
    "for carrier in abbr_companies.keys():\n",
    "    fg1 = flights[flights['AIRLINE'] == carrier]\n",
    "    test = fg1['ARRIVAL_DELAY'].groupby(flights['ORIGIN_AIRPORT']).apply(get_stats).unstack()\n",
    "    airport_mean_delays[carrier] = test.loc[:, 'mean'] \n",
    "sns.set(context=\"paper\")\n",
    "fig = plt.figure(1, figsize=(12,12))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "subset = airport_mean_delays.iloc[:40,:].rename(columns = abbr_companies)\n",
    "subset = subset.rename(index = identify_airport)\n",
    "mask = subset.isnull()\n",
    "sns.heatmap(subset, linewidths=0.05, cmap=\"YlGnBu\", mask=mask, vmin = 0, vmax = 30)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=12, rotation = 88) ;\n",
    "ax.yaxis.label.set_visible(False)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)    \n",
    "subset = airport_mean_delays.iloc[40:80,:].rename(columns = abbr_companies)\n",
    "subset = subset.rename(index = identify_airport)\n",
    "fig.text(0.5, 1.02, \"Scale of Delays From Origin City\", ha='center', fontsize = 20)\n",
    "mask = subset.isnull()\n",
    "sns.heatmap(subset, linewidths=0.05, cmap=\"YlGnBu\", mask=mask, vmin = 0, vmax = 30)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=12, rotation = 88) ;\n",
    "ax.yaxis.label.set_visible(False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f8bd257-a4ed-4074-978d-98f6c3ca1bfb",
    "_uuid": "615892736008225f0fa1d33115d56d33afd02c5b"
   },
   "source": [
    "#### Analysis: The delays are highly dependent with cities. For example, Frontier Airlines Inc. has high delay in Chicago compare to other cities.\n",
    "<a id=\"24\"></a>\n",
    "### 2.4 Delays Distribution\n",
    "\n",
    "I plot this stripe plot to get a macrostatistics of arrival delays for airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5d56ab4-9b43-40b3-ad1e-a5dcabc86623",
    "_uuid": "7339ca20ff0c830f674658b0af3be18ed4c6c19a"
   },
   "outputs": [],
   "source": [
    "df2 = flights.loc[:, ['AIRLINE', 'ARRIVAL_DELAY']]\n",
    "df2['AIRLINE'] = df2['AIRLINE'].replace(abbr_companies)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "colors = ['firebrick', 'gold', 'lightcoral', 'aquamarine', 'c', 'yellowgreen', 'grey',\n",
    "          'seagreen', 'tomato', 'violet', 'wheat', 'chartreuse', 'lightskyblue', 'royalblue']\n",
    "ax = sns.stripplot(y=\"AIRLINE\", x=\"ARRIVAL_DELAY\", size = 1.7, palette = colors,\n",
    "                    data=df2, linewidth = 0.3,  jitter=True)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=14)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=14)\n",
    "ax.set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*[int(y) for y in divmod(x,60)])\n",
    "                         for x in ax.get_xticks()])\n",
    "plt.xlabel('ARRIVAL DELAY', fontsize=18, bbox={'facecolor':'midnightblue', 'pad':5},\n",
    "           color='w', labelpad=20)\n",
    "ax.yaxis.label.set_visible(False)\n",
    "ax.set_title('Flights Delays for Different Airlines',size=30)\n",
    "\n",
    "plt.show()\n",
    "del df2 , abbr_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3bbb2829-168a-45d3-a9f6-e6d87ddb25e4",
    "_uuid": "4511fce6966df8d1ebf8d9348b4fea44998d87eb"
   },
   "source": [
    "#### Analysis:  There are a lot of flights from American Airlines which have high arrival delay(Even bigger than 18h). However, the delays of flights from Hawaiian Airlines Inc. are mainly within 6h, which is very good.\n",
    "\n",
    "<a id=\"25\"></a>\n",
    "### 2.5 Delays Type\n",
    "\n",
    "To see more details, I divide arrival dalays into two different types based on my own experience.\n",
    "\n",
    "The decision is that the arrival delays which exceed 45 minutes are \"Seriously Delay\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "503cced1-5d71-4f7c-80de-6dd0aa03cff6",
    "_uuid": "fca30ec0bad75b75003d2c1f71e998036a6f51b9"
   },
   "outputs": [],
   "source": [
    "flights['DELAY_LEVEL'] = flights['ARRIVAL_DELAY'].apply(lambda x:(0,1)[x > 45])\n",
    "zero_list = []\n",
    "one_list = []\n",
    "for col in list(flights['AIRLINE'].unique()):\n",
    "    zero_list.append(flights[flights['AIRLINE']==col].groupby('DELAY_LEVEL').count()['YEAR'][0])\n",
    "    one_list.append(flights[flights['AIRLINE']==col].groupby('DELAY_LEVEL').count()['YEAR'][1])\n",
    "trace1 = go.Bar(\n",
    "    x=list(flights['AIRLINE'].unique()),\n",
    "    y=zero_list ,\n",
    "    name='Not Seriously Delay'\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=list(flights['AIRLINE'].unique()),\n",
    "    y=one_list,\n",
    "    name='Seriously Delay'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    barmode='stack',\n",
    "    title='Count of flights Based on Delay Type',\n",
    "    titlefont={\"size\": 36}\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17c4b6c2-3fbc-4032-9b21-03c7584f3390",
    "_uuid": "9c5290592f5117928d8cd7d31a9eed3f2c754afd"
   },
   "source": [
    "#### Analysis:  We can find that flights from Southwest Airlines Co. have highest possibility to cause seriously delay. The airlines which have few flights count commonly have low chance to cause seriously delay.\n",
    "\n",
    "<a id=\"26\"></a>\n",
    "### 2.6 Speed Analysis\n",
    "\n",
    "We can figure out that the relation between arrival delay and flight speed using the violin plot.\n",
    "\n",
    "Here, I used this euqation: **v = s/t** (v = speed, s = distance and t = time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f45aa71-b0b3-4272-beaa-2beed53e53a8",
    "_uuid": "f08dff11a06b4db94e282867a31ab10e738eb5f4"
   },
   "outputs": [],
   "source": [
    "flights['FLIGHT_SPEED'] = 60*flights['DISTANCE']/flights['AIR_TIME']\n",
    "flights['DELAY_LEVEL_NAME'] = flights['DELAY_LEVEL'].apply(lambda x:('Not Seriously Delay','Seriously Delay')[x == 1])\n",
    "fig= plt.figure(figsize=(12,9))\n",
    "ax = sns.violinplot(data=flights[flights['FLIGHT_SPEED']<100000], y='AIRLINE',x='FLIGHT_SPEED',hue='DELAY_LEVEL_NAME')\n",
    "ax.set_title('Flight Speed for Different Airlines',size=30)\n",
    "ax.set_ylabel('Airline',size=20)\n",
    "ax.set_xlabel('Flight Speed (miles/hour)',size=20)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='20')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac919505-db40-4f40-a665-42e973c1310a",
    "_uuid": "9be26c6c82022b3f97b2ad6a6cfd671c3913fcf8"
   },
   "source": [
    "#### Analysis:  Basicly, flights causing seriously delay is slower, which means that the speed is a huge influence factor that lead to delay. The exception is flights from Hawaiian Airlines Inc., but considering that flights count of  Hawaiian Airlines Inc. is very low(only 1.31% in total) we are still right.\n",
    "\n",
    "<a id=\"27\"></a>\n",
    "### 2.7 Build and Train Model\n",
    "\n",
    "Now, I will build a model to predict the arrival delays. First, I need to choose the right data from the origin dataset, since many of them indicates the answer. For example, the AIR_TIME represent the whole flying time, which you can get the arrival delays by substraction. I choose the data which contains different delays except departure delay according to the relation heatmap above. After using grid search to tune the best parameters for this lightGBM model on my own computer, I can build the model properly and fit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5bd192aa-7ee8-4fa7-9205-14c8aebc6c56",
    "_uuid": "3e134ed23650c678626d524b5606ab63cddca64e"
   },
   "outputs": [],
   "source": [
    "flights = pd.read_csv('../input/flight-delays/flights.csv',low_memory=False)\n",
    "flights = flights[~flights['AIR_SYSTEM_DELAY'].isnull()]\n",
    "target = flights.ARRIVAL_DELAY.values.copy()\n",
    "y = np.log1p(target)\n",
    "feature_columns = ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER',\n",
    "       'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n",
    "       'SCHEDULED_DEPARTURE','SCHEDULED_TIME', 'DISTANCE','SCHEDULED_ARRIVAL', 'LATE_AIRCRAFT_DELAY',\n",
    "       'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'WEATHER_DELAY']\n",
    "flights = flights[feature_columns]\n",
    "cat_col = ['AIRLINE','TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']\n",
    "def encoder(col_name):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(flights[col_name].astype(str).values)\n",
    "for col in cat_col:\n",
    "    flights[col] = encoder(col)\n",
    "Xt, Xv, yt, yv = train_test_split(flights, y, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "       'learning_rate': 0.75,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 1000,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE'\n",
    "}\n",
    "evals_result = {} \n",
    "d_train = lgb.Dataset(Xt, label=yt)\n",
    "d_valid = lgb.Dataset(Xv, label=yv)\n",
    "model = lgb.train(params, d_train, 6000, valid_sets=[d_valid], verbose_eval=1000,evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "043b5ff4-1d3c-4363-b34d-eff702b2ee0e",
    "_uuid": "ca440bf171e1a239117dadbfe8de02c826641298"
   },
   "source": [
    "<a id=\"28\"></a>\n",
    "### 2.8 Model Evaluation\n",
    "\n",
    "Evaluate the performance of the regression model based on the test set.\n",
    "\n",
    "Here, I use **Root Mean Squared Logarithmic Error** as the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f2e00cfe-7e17-4e2d-a952-3c9a30f6ca28",
    "_uuid": "c3a260ea4ec0e4434538e5db03d4426d881b3151"
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "y_preds = model.predict(Xv)\n",
    "y_preds = np.expm1(y_preds)\n",
    "y_true = np.expm1(yv)\n",
    "v_rmsle = rmsle(y_true, y_preds)\n",
    "print(\" RMSLE error on dev test: \"+str(v_rmsle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f3929c5-ffdb-43bc-9533-3c06ef46aada",
    "_uuid": "35314bdc0915656dcf368b549ea52bbf44bb0abf"
   },
   "source": [
    "#### Analysis:  The RMSLE is really low(<0.01), meaning that our model is very successful. We can  accurately predict arrival delay based on the provided information. In other words, we can decrease arrival delay according to the model.\n",
    "\n",
    "<a id=\"29\"></a>\n",
    "### 2.9 Factor Analysis and Suggestion\n",
    "\n",
    "In order to decreasing the delay, let's see the influence of different factors, and decide the strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "057866dc-98a9-4616-91fd-7bfabb5fe4ef",
    "_uuid": "fe95acd68ad3100a0c89919d2419f25387c48ce6"
   },
   "outputs": [],
   "source": [
    "ax = lgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7460fd9d-9d5d-4b26-ba2a-e4d3c02cb4c0",
    "_uuid": "516ba658d48671af7f33752c1607ba1631de1f66"
   },
   "source": [
    "#### Analysis: Late aircraft delay Airline delay is the most important factors. Airline company should invest more money on reducing late aircraft delay while ignoring security delay which is unimportant.\n",
    "<a id=\"3\"></a>\n",
    "___\n",
    "## 3. Twitter Sentiment Analysis\n",
    "___\n",
    "<a id=\"31\"></a>\n",
    "### 3.1 Sentiment Distribution\n",
    "\n",
    "Let's see the overall information for the Twitter sentiments for different airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c924b50d-a6f1-4787-a26a-e1ec00b0e3d1",
    "_uuid": "7451d983847e2471c6708136a799cec3b0fca12f"
   },
   "outputs": [],
   "source": [
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "for col in list(tweet['airline'].unique()):\n",
    "    positive_list.append(tweet[tweet['airline']==col].groupby('airline_sentiment').count()['tweet_id'].positive)\n",
    "    negative_list.append(tweet[tweet['airline']==col].groupby('airline_sentiment').count()['tweet_id'].negative)\n",
    "    neutral_list.append(tweet[tweet['airline']==col].groupby('airline_sentiment').count()['tweet_id'].neutral)\n",
    "trace1 = go.Bar(\n",
    "    x=list(tweet['airline'].unique()),\n",
    "    y=positive_list ,\n",
    "    name='Positive Sentiment'\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=list(tweet['airline'].unique()),\n",
    "    y=negative_list,\n",
    "    name='Negative Sentiment'\n",
    ")\n",
    "trace3 = go.Bar(\n",
    "    x=list(tweet['airline'].unique()),\n",
    "    y=neutral_list,\n",
    "    name='Neutral Sentiment'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "layout = go.Layout(\n",
    "    barmode='stack',\n",
    "    title='Customer sentiment for different airlines',\n",
    "    titlefont={\"size\": 36}\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4749e548-e2be-4719-89f5-8e5d27e806e8",
    "_uuid": "8c3c3dd0893f39ca2d13289d41f5eb5fafd9628c"
   },
   "source": [
    "#### Analysis: United airlines are the most popular airline, receiving most comment, while few people choose Virgin America. Delta has highest quality of service because of the percentage of positive feedback.\n",
    "\n",
    "<a id=\"32\"></a>\n",
    "### 3.2 Word Cloud for Positive Sentiment\n",
    "\n",
    "What will people say when they have positive sentiment for this airline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "981caa93-eff7-4d60-be1a-a50038da22f6",
    "_uuid": "bc5d3a60941431c09c1a3ad93985872dbc045439"
   },
   "outputs": [],
   "source": [
    "X = tweet['text']\n",
    "y = tweet['airline_sentiment']\n",
    "negative = y == 'negative'\n",
    "positive = y == 'positive'\n",
    "neutral = y == 'neutral'\n",
    "stopwords = set(STOPWORDS)\n",
    "def create_wordcloud(subset):\n",
    "    plt.figure(figsize=(20,15))\n",
    "    wc = WordCloud(background_color=\"white\", max_words=2000, \n",
    "                   stopwords=stopwords, max_font_size= 40)\n",
    "    wc.generate(\" \".join(tweet[subset]['text'].str.lower()))\n",
    "    plt.imshow(wc.recolor(colormap='ocean', random_state=37), alpha=0.98, interpolation=\"bilinear\")\n",
    "    plt.axis('off');\n",
    "    \n",
    "stopwords.update(['flight','delta','jetblue','americanair','usairway','southwestair','usairways',\n",
    "                  'united','virginamerica','southwest'])\n",
    "create_wordcloud(positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c8c9388-b2c8-4d45-9576-74934dd23322",
    "_uuid": "736e5f47177f54ba9e878ab1bff893a7f41b1685"
   },
   "source": [
    "#### Analysis:  Thank, great, awesome, good, appreciate and amazing are popular when people have positive sentiment. The customers are tended to share their good mood after enjoying a high quality flying trip.\n",
    "\n",
    "<a id=\"33\"></a>\n",
    "### 3.3 Word Cloud for Negative Sentiment\n",
    "\n",
    "Now, Let's see the common word when people think the service is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "70766267-2f1f-45af-802c-7d9346449ad0",
    "_uuid": "552b152839c423cd44fb7e243ec7fe23ce39a6ec"
   },
   "outputs": [],
   "source": [
    "create_wordcloud(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7aa8eeab-acfe-4d13-a0a2-809272e67749",
    "_uuid": "9e7e20e0c6f1fdb57dcc867f4e9e65106c3016f8"
   },
   "source": [
    "#### Analysis:  We can find that the Tweets with negative moods are frequently involved some words like cancelled, flight ,customer or hour. People might guess that customer tends to complain when they are waiting for the delayed flights.\n",
    "\n",
    "<a id=\"34\"></a>\n",
    "### 3.4 Word Cloud for Neutral Sentiment\n",
    "\n",
    "We can also get the similarities among nueutral sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ce7f923-2198-4305-a21c-9fc9bceacaa4",
    "_uuid": "29c1339097981a9845fac48cc0f3355c564b63bf"
   },
   "outputs": [],
   "source": [
    "create_wordcloud(neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ba7e1af-35bb-4df0-bdaa-05f1b93411bf",
    "_uuid": "d8e1d23bb4dae42e4d407fdae8762ca853db4ab4"
   },
   "source": [
    "#### Analysis:  \"Thank\" is still popular in neutral sentiment, while other common words are mostly nouns such as today, tomorrow and ticket. I think people will try the airline again if they have neutral sentiment.\n",
    "\n",
    "<a id=\"35\"></a>\n",
    "### 3.5 Build and Train RNN Model\n",
    "\n",
    "I take the tweet text as the main input while taking other factor such as tweet created time as axillary input to construct the model. The following picture is the exact structure of my RNN model.\n",
    "![](https://lh3.googleusercontent.com/5wfEY_1v2gFB9ywBTHpb22dfvFR61MDW1aDhyu8l194Nl2x40KsKCAGUIgsEJQjHkhzIVb2--Q1Bwyn8Rj1CnX3K6jRc84blVWxuzwCjDUa_ofVtWV-NdKGLJP5aPiI2vWQdTMWh3yKbY0HJcDQ9W5QhvdNBVAOqM18xx8UAGrkPIkRK3DRK78mf6JpSkCPv8pLnnoL0Cq7ww69iDEKVopo3X4EdhDoAafUi1cgMZhyDsvFGmOC3-Z-5dgyyTXp0Ug0fSb7jigbhVIKtPP1fEnMIdVYY2PyHR3x67L-k3vbtpB8TVzXHqAe-VqeiwY5QKhP4LW4yg6noDCMGBY0_BKiZ47CIb7WC_2Wj4xIyDdIY8mQOmGT29SfK0ccUqn3YNpzkXi0LyGfgr1yljXq0HjyZedrzbL7DA1qd-SnmPgwP-lGfyVi1a3KhuMkaDYCdAg2-UtUdnQ8Y0F3SS-IQth4mDxsT3VQ-1wcgjWCnkBjyOd7F88I-Pb8W-SADBKIFsf8RddcN4kP3MEUIgO34lwkL3czQY8-91tdPkXraoWqqf9qAG_a2bwiQ2GbfkvaoydhfmItI1dBPRU3DM9WiompXZjfp9CPI1mGwx05I=w674-h621-no)\n",
    "\n",
    "Then, get the best parameters using grid search and train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ed7c85f-bb01-4514-9090-abd19c8e632a",
    "_uuid": "5d7d2e4a294b9811ebabb08a8d1374ed684232b3"
   },
   "outputs": [],
   "source": [
    "tweet['Created_Month'] = tweet['tweet_created'].apply(lambda x: x.month)\n",
    "tweet['Created_Day'] = tweet['tweet_created'].apply(lambda x: x.day)\n",
    "tweet = tweet.drop(['tweet_id','airline_sentiment_gold','negativereason_gold','tweet_coord','name','tweet_created'],axis=1)\n",
    "def handle_missing(dataset):\n",
    "    dataset.negativereason.fillna(value=\"None\", inplace=True)\n",
    "    dataset.negativereason_confidence.fillna(value=0, inplace=True)\n",
    "    dataset.tweet_location.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.user_timezone.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "tweet = handle_missing(tweet)\n",
    "def tweet_encoder(col_name):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(tweet[col_name])\n",
    "cat_columns = ['airline_sentiment', 'negativereason',\n",
    "               'airline','tweet_location', 'user_timezone']\n",
    "for col in cat_columns:\n",
    "    tweet[col] = tweet_encoder(col)\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(tweet['text'])\n",
    "tweet[\"text\"] = tok_raw.texts_to_sequences(tweet.text.str.lower())\n",
    "dtrain, dvalid = train_test_split(tweet, random_state=123, train_size=0.8)\n",
    "dtrain_target = pd.get_dummies(dtrain.airline_sentiment).values\n",
    "dvalid_target = pd.get_dummies(dvalid.airline_sentiment).values\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'text': pad_sequences(dataset.text, maxlen=36),\n",
    "        'airline_sentiment_confidence': np.array(dataset.airline_sentiment_confidence),\n",
    "        'negativereason' : np.array(dataset.negativereason),\n",
    "         'negativereason_confidence' : np.array(dataset.negativereason_confidence),\n",
    "         'airline' : np.array(dataset.airline),\n",
    "         'retweet_count' : np.array(dataset.retweet_count),\n",
    "         'tweet_location' : np.array(dataset.tweet_location),\n",
    "        'user_timezone' : np.array(dataset.user_timezone),\n",
    "        'Created_Month' : np.array(dataset.Created_Month),\n",
    "        'Created_Day' : np.array(dataset.Created_Day),\n",
    "    }\n",
    "    return X\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "MAX_TEXT = np.max([np.max(tweet.text.max())])+80\n",
    "MAX_NEGATIVEREASON = np.max([tweet.negativereason.max()])+1\n",
    "MAX_AIRLINE = np.max([tweet.airline.max()])+1\n",
    "MAX_LOCATION = np.max([tweet.tweet_location.max()])+1\n",
    "MAX_TIMEZONE = np.max([tweet.user_timezone.max()])+1\n",
    "def get_model():\n",
    "    dr_r = 0.5\n",
    "    \n",
    "    text = Input(shape=[X_train[\"text\"].shape[1]], name=\"text\")\n",
    "    airline_sentiment_confidence = Input(shape=[1], name=\"airline_sentiment_confidence\")\n",
    "    negativereason = Input(shape=[1], name=\"negativereason\")\n",
    "    negativereason_confidence = Input(shape=[1], name=\"negativereason_confidence\")\n",
    "    airline = Input(shape=[1], name=\"airline\")\n",
    "    retweet_count = Input(shape=[1], name=\"retweet_count\")\n",
    "    tweet_location = Input(shape=[1], name=\"tweet_location\")\n",
    "    user_timezone = Input(shape=[1], name=\"user_timezone\")\n",
    "    Created_Month = Input(shape=[1], name=\"Created_Month\")\n",
    "    Created_Day = Input(shape=[1], name=\"Created_Day\")\n",
    "    \n",
    "    emb_text = Embedding(MAX_TEXT, 50)(text)\n",
    "    emb_negativereason = Embedding(MAX_NEGATIVEREASON, 10)(negativereason)\n",
    "    emb_airline = Embedding(MAX_AIRLINE, 10)(airline)\n",
    "    emb_tweet_location = Embedding(MAX_LOCATION, 10)(tweet_location)\n",
    "    emb_user_timezone = Embedding(MAX_TIMEZONE, 10)(user_timezone)\n",
    "\n",
    "    rnn_layer = GRU(128) (emb_text)\n",
    "    \n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_negativereason)\n",
    "        , Flatten() (emb_airline)\n",
    "        , Flatten() (emb_tweet_location)\n",
    "        , Flatten() (emb_user_timezone)\n",
    "        , rnn_layer\n",
    "        , Created_Day\n",
    "        , Created_Month\n",
    "        , retweet_count\n",
    "        , negativereason_confidence\n",
    "        , airline_sentiment_confidence\n",
    "    ])\n",
    "    main_l = BatchNormalization() (Dropout(dr_r) (Dense(64) (main_l)))\n",
    "    main_l = BatchNormalization() (Dropout(dr_r) (Dense(64) (main_l)))\n",
    "    main_l = BatchNormalization() (Dropout(dr_r) (Dense(64) (main_l)))\n",
    "\n",
    "    output = Dense(3, activation=\"softmax\") (main_l)\n",
    "    \n",
    "    model = Model([text, airline_sentiment_confidence, negativereason, negativereason_confidence,\n",
    "    airline, retweet_count, tweet_location, user_timezone, Created_Month, Created_Day], output)\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model    \n",
    "model = get_model()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "epochs = 9\n",
    "\n",
    "model = get_model()\n",
    "model.fit(X_train, dtrain_target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_valid, dvalid_target)\n",
    "          , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e067ecb-8f85-4904-a236-9b7ec13ef3af",
    "_uuid": "3e78d3e9c7195552c17f7da30d451f9e09ab9c0d"
   },
   "source": [
    "#### Analysis:  The accuracy is quiet high(84.6%), meaning that we can predict the sentiment from people's tweets. We can build an associating recommendation system for those people who want to take a plane and make sure they will satisfy this trip, improving the service.  \n",
    "<a id=\"4\"></a>\n",
    "___\n",
    "## Conclusion\n",
    "___\n",
    "This notebook is three-fold. The first part deals with an exploration of the dataset, with the aim of understanding some properties of flights distribution across US. This exploration gives me the occasion of using various vizualization tools offered by python. The second part of the notebook consists in the elaboration of a model aimed at predicting flight arrival delays. For that purpose, I use lightGBM model and show the importance of different factors. The model is so accurate that we could reduce the flights delays according to the results. The third part deals with Twitter tweets and the RNN model performs really well, meaning that there must be some pattern in customer tweets. We can build a recommendation system to improve customer feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
